# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17OTbsVq6HlUk4UGv2M5uIDcWwQosltLu
"""

pip install nltk

import nltk

nltk.download('punkt')

import tensorflow as tf
import numpy as np
import keras
import tflearn
import random
import json
import pickle
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import SGD

from nltk.stem.lancaster import LancasterStemmer
stemmer= LancasterStemmer()

with open("/content/drive/My Drive/intents.json") as file:
  data= json.load(file)

print(data)

#try:
 # with open("data.pickle", "rb") as f:
  #  words,labels,training,output=pickle.load(f)
#except:    
  words=[]
  labels=[]
  docs_x=[]
  docs_y=[]
  for intent in data["intents"]:
    for pattern in intent["patterns"]:
      wrd=nltk.word_tokenize(pattern)
      words.extend(wrd)
      docs_x.append(wrd)
      docs_y.append(intent["tag"])
    if intent["tag"] not in labels:
      labels.append(intent["tag"])
  words=[stemmer.stem(w.lower()) for w in words if w!="?"]
  words=sorted(list(set(words)))  
  labels=sorted(labels)
  training=[]
  output=[]
  out_empty=[0 for _ in range(len(labels))]
  for x,doc in enumerate(docs_x):
    bag=[]
    wrds=[stemmer.stem(w) for w in doc]
    for w in words:
      if w in wrds:
        bag.append(1)
      else:
        bag.append(0)
    output_row=out_empty[:]
    output_row[labels.index(docs_y[x])]=1
    training.append(bag)
    output.append(output_row)
  training= np.array(training)
  output=np.array(output)  
  
 # with open("data.pickle","wb") as f:
  #  pickle.dump((words,labels,training,output),f)

model = Sequential()
model.add(Dense(128, input_shape=(len(training[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(output[0]), activation='softmax'))
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

hist = model.fit(training, output, epochs=200, batch_size=5, verbose=1)
model.save('chatbott', hist)



def bag_of_words(s, words):
    bag = [0 for _ in range(len(words))]

    s_words = nltk.word_tokenize(s)
    s_words = [stemmer.stem(word.lower()) for word in s_words]

    for se in s_words:
        for i, w in enumerate(words):
            if w == se:
                bag[i] = 1
            
    return np.array(bag)


def chat():
    print("Start chatting with your AI Gf!")
    while True:
        inp = input("You: ")
        if inp.lower() == "quit":
            break
        p=bag_of_words(inp,words)  
        results = model.predict(np.array([p]))[0]
        results_index = np.argmax(results)
        tag = labels[results_index]
        if results[results_index]>0.65:
          for tg in data["intents"]:
              if tg['tag'] == tag:
                  responses = tg['responses']
          
          print("AI friend:",random.choice(responses))
        else:
          print("Sorry, I don't get that")  

chat()
   
